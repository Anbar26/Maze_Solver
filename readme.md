# Maze Solver

**Reinforcement Learningâ€“Based Maze Navigation**

> Monte Carlo, SARSA, and Q-Learning approaches for autonomous maze solving.

---

## ðŸ—ºï¸ Overview

**Maze Solver** is a reinforcement learning project that explores how an agent learns to navigate a maze environment through trial-and-error.
The maze is modeled as a **Markov Decision Process (MDP)**, and the project implements and compares three core reinforcement learning algorithms:

* **Q-Learning** â€“ off-policy Temporal Difference control.
* **SARSA** â€“ on-policy Temporal Difference control.
* **Monte Carlo (MC)** â€“ learning from complete episodes.

This progression illustrates how reinforcement learning evolves from **experience-based** to **bootstrapped** and **off-policy** learning.

---

## ðŸŽ¯ Project Objectives

* Model maze navigation as an MDP.
* Implement and compare Monte Carlo, SARSA, and Q-Learning algorithms.
* Analyze convergence behavior and policy optimality.
* Visualize how exploration (Îµ-greedy) influences learning outcomes.

---

## ðŸ§© Core Concepts

### **Markov Decision Process (MDP)**

The maze is formulated as an MDP defined by:
âŸ¨S, A, P, R, Î³âŸ©

* **S** â€” set of states (grid positions)
* **A** â€” set of actions (up, down, left, right)
* **P** â€” transition probabilities
* **R** â€” reward function
* **Î³** â€” discount factor

The objective is to learn an optimal policy Ï€* that maximizes expected cumulative reward.

---

## âš™ï¸ Algorithms Implemented

### ðŸ§® 1. Monte Carlo (MC)

**Idea:** Learn from *complete episodes* by averaging returns.

* Updates value estimates only at the end of each episode.
* No bootstrapping â†’ high variance but unbiased estimates.

**Update rule:**

> `Q(s,a) â† Q(s,a) + Î± [G_t âˆ’ Q(s,a)]`
> where `G_t` is the total return from time *t* onward.

**Use case:**
Excellent for illustrating learning from experience when the environment is episodic and model-free.

---

### ðŸ” 2. SARSA (On-Policy TD Control)

**Idea:** Learns the action-value function while *following* the same policy used for acting (on-policy).

* Bootstraps from the next stateâ€™s estimated Q-value.
* Sensitive to exploration strategy (Îµ-greedy).

**Update rule:**

> `Q(s,a) â† Q(s,a) + Î± [r + Î³Q(sâ€²,aâ€²) âˆ’ Q(s,a)]`

**Use case:**
More stable than Q-Learning in stochastic environments because it learns the policy it actually follows.

---

### âš¡ 3. Q-Learning (Off-Policy TD Control)

**Idea:** Learns the *optimal* policy independent of the agentâ€™s behavior.

* Bootstraps using the max Q-value of the next state.
* Off-policy â†’ uses greedy target updates even during exploratory behavior.

**Update rule:**

> `Q(s,a) â† Q(s,a) + Î± [r + Î³ maxâ‚â€² Q(sâ€²,aâ€²) âˆ’ Q(s,a)]`

**Use case:**
Fast convergence to optimal policy; ideal for deterministic or well-defined environments.

---

## ðŸ§  Exploration Strategy â€” Îµ-Greedy

To balance exploration and exploitation, all three algorithms use **Îµ-greedy action selection**:

> * With probability **Îµ**, choose a random action (explore).
> * With probability **1âˆ’Îµ**, choose the best action (exploit).

Îµ decays gradually over time to shift from exploration to exploitation as learning progresses.

---

## ðŸ§ª Environment Setup

* **Grid-based maze**: start state, goal state, and obstacles.
* **Actions**: Up, Down, Left, Right.
* **Rewards**:

  * `+10` â†’ goal reached
  * `âˆ’1` â†’ step penalty
  * `âˆ’5` â†’ collision with wall

---

## ðŸ“ˆ Metrics and Visualization

Each algorithm tracks:

* Average reward per episode
* Success rate (% of episodes reaching goal)
* Steps per episode
* Convergence of Q-values

Visualizations include:

* Learning curve (reward vs episodes)
* Heatmap of learned policy
* Trajectory visualization of the final policy

---

## ðŸ“ Repository Structure

```
Maze_Solver/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ envs/
â”‚   â”‚   â””â”€â”€ maze_env.py
â”‚   â”œâ”€â”€ agents/
â”‚   â”‚   â”œâ”€â”€ q_learning.py
â”‚   â”‚   â”œâ”€â”€ sarsa.py
â”‚   â”‚   â””â”€â”€ monte_carlo.py
â”‚   â”œâ”€â”€ app.py
â”‚   â””â”€â”€ requirements.txt
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ components/
â”‚   â””â”€â”€ package.json
â”œâ”€â”€ documentation/
â””â”€â”€ README.md
```

---

## ðŸš€ Quick Start

### Backend Setup
```bash
cd backend
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
pip install -r requirements.txt
uvicorn app:app --reload
```

### Frontend Setup
```bash
cd frontend
pnpm install
pnpm dev
```

### Access the Application
- Frontend: http://localhost:3000
- Backend API: http://localhost:8000/docs

### Train Agents via Web UI
1. Select algorithm (Q-Learning, SARSA, or Monte Carlo)
2. Configure hyperparameters (episodes, alpha, gamma, epsilon)
3. Click "Start Training"
4. View real-time training logs and policy visualization
5. Simulate the learned policy

---

## ðŸ“Š Comparative Summary

| Algorithm       | Type     | Policy     | Bootstraps | Sample Efficiency | Variance | Notes                              |
| --------------- | -------- | ---------- | ---------- | ----------------- | -------- | ---------------------------------- |
| **Monte Carlo** | Episodic | On-policy  | âŒ          | Low               | High     | Learns from full episodes only     |
| **SARSA**       | TD       | On-policy  | âœ…          | Moderate          | Moderate | Safer in stochastic environments   |
| **Q-Learning**  | TD       | Off-policy | âœ…          | High              | Moderate | Converges faster to optimal policy |

---

## ðŸ§© Future Work

* Extend to **TD(Î»)** or **n-step TD** for smoother convergence.
* Implement **Boltzmann (Softmax) exploration** as an alternative to Îµ-greedy.
* Use **neural function approximation (DQN)** for larger mazes.

---

## ðŸ‘¨â€ðŸ’» Authors

**Arhaan Girdhar  --> (220962050)**
</br>
**Anbar Althaf   ----->  (220962051)**
</br>
*CSE 4478 â€“ Reinforcement Learning*
</br>
Department of Computer Science and Engineering ( AI & ML )

---

## ðŸ“œ License

MIT License â€” free for academic and research use.

---
